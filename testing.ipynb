{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import utils\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../DeepSplines'))\n",
    "from deepsplines.ds_modules.deepBspline import DeepBSpline\n",
    "from models import LinearBSpline\n",
    "\n",
    "\n",
    "opt_params = {\n",
    "        'size': 3,\n",
    "        'range_': 1,\n",
    "        'init': 'relu',\n",
    "        'save_memory': False\n",
    "}\n",
    "\n",
    "bspline = DeepBSpline('fc', 8, **opt_params) # create a single BSpline Layer\n",
    "\n",
    "locations = bspline.grid_tensor.detach()\n",
    "coefficients = bspline.coefficients_vect.view(bspline.num_activations, bspline.size).detach()\n",
    "\n",
    "# print(locations)\n",
    "# print(coefficients)\n",
    "\n",
    "# utils.plot_bspline(locs=locations[0].numpy(), coeffs=coefficients[0].numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'linspline' has no attribute 'LinearSpline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m      8\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(linspline)\n\u001b[0;32m---> 10\u001b[0m linspline \u001b[38;5;241m=\u001b[39m \u001b[43mlinspline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinearSpline\u001b[49m(locations[\u001b[38;5;241m0\u001b[39m], coefficients[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m (locs, coeffs) \u001b[38;5;241m=\u001b[39m linspline\u001b[38;5;241m.\u001b[39mget_locs_coeffs()\n\u001b[1;32m     14\u001b[0m x_plot \u001b[38;5;241m=\u001b[39m x_plot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m400\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'linspline' has no attribute 'LinearSpline'"
     ]
    }
   ],
   "source": [
    "import linspline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(linspline)\n",
    "\n",
    "linspline = linspline.LinearSpline(locations[0], coefficients[0])\n",
    "\n",
    "(locs, coeffs) = linspline.get_locs_coeffs()\n",
    "\n",
    "x_plot = x_plot = torch.from_numpy(np.linspace(-1, 1, 400))\n",
    "y_plot = linspline.forward(x_plot)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_plot, y_plot, label='Linear Spline', color='blue')\n",
    "plt.scatter(locs, coeffs, color='red', label='Knot Points')\n",
    "plt.title('Linear Spline Interpolation using NumPy.interp')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy model to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearBSpline([8], 3, 1, 'relu') # create a BSpline based model\n",
    "\n",
    "for layer in range(len(model.get_deepspline_activations())):\n",
    "    locs = model.get_deepspline_activations()[layer]['locations']\n",
    "    coeffs = model.get_deepspline_activations()[layer]['coefficients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! test timing on single bspline vs. linear spline vs. relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "\n",
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (1): LinearSplineLayer(locs=Tensor(shape=(8, 3)), coeffs=Tensor(shape=(8, 3)), mode='fc')\n",
      "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import linspline\n",
    "import importlib\n",
    "\n",
    "importlib.reload(linspline)\n",
    "\n",
    "lin_model = linspline.LSplineFromBSpline(model.get_layers())\n",
    "\n",
    "print(\"\\n----\\n\")\n",
    "print(lin_model.get_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss on epoch 0: 0.9271897673606873\n",
      "Val loss on epoch 5: 0.9456093311309814\n",
      "Val loss on epoch 10: 0.6865884065628052\n",
      "Val loss on epoch 15: 1.0029758214950562\n",
      "Val loss on epoch 20: 0.6661977767944336\n",
      "Val loss on epoch 25: 0.6564297080039978\n",
      "Val loss on epoch 30: 0.859581470489502\n",
      "Val loss on epoch 35: 0.5596232414245605\n",
      "Val loss on epoch 40: 0.5278910398483276\n",
      "Val loss on epoch 45: 0.5143812894821167\n",
      "Val loss on epoch 0: 0.5405814051628113\n",
      "Val loss on epoch 5: 0.506903886795044\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from omegaconf import OmegaConf, ListConfig\n",
    "\n",
    "from models import LinearReLU, LinearBSpline\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import string, random, os\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def to_dict(self):\n",
    "        def convert(value):\n",
    "            if isinstance(value, ListConfig):\n",
    "                return OmegaConf.to_object(value)\n",
    "            return value\n",
    "\n",
    "        return {key: convert(value) for key, value in self.__dict__.items()}\n",
    "\n",
    "def training_run(mparams, tparams, X, y):\n",
    "\n",
    "    # train-test split of the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, shuffle=True)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # using a dataloader to randomize batching\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tparams.batch_size, shuffle=True)\n",
    "    \n",
    "    loss_fn = nn.MSELoss()  # mean square error\n",
    "    cr_fwd_lat = -1\n",
    "\n",
    "    model_save_code = ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n",
    "    \n",
    "    for arch in [\"relu\", \"bspline\"]:\n",
    "        \n",
    "        # set specifications based on the architecture\n",
    "        if(arch == \"relu\"):\n",
    "            if(tparams.relu_epochs == 0):\n",
    "                continue\n",
    "            model = LinearReLU(mparams.layers)\n",
    "            epochs = tparams.relu_epochs\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=tparams.lr_wb)\n",
    "            \n",
    "            if(tparams.lrs == \"steplr\"):\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=tparams.lrs_stepsize, gamma=tparams.lrs_gamma)\n",
    "\n",
    "        elif(arch == \"bspline\" or arch == \"both\"):\n",
    "            if(arch == \"bspline\"):\n",
    "                if(tparams.spline_epochs == 0):\n",
    "                    continue\n",
    "                if(tparams.relu_epochs > 0): # if we pretrained on ReLU, load those weights\n",
    "                    model = LinearBSpline(mparams.layers, mparams.cpoints, mparams.range_)\n",
    "                    model.load_state_dict(torch.load(f\"./temp_models/{model_save_code}.pt\", weights_only=True), strict=False)\n",
    "                else:\n",
    "                    model = LinearBSpline(mparams.layers, mparams.cpoints, mparams.range_)\n",
    "                epochs = tparams.spline_epochs\n",
    "\n",
    "            if(arch == \"both\"):\n",
    "                if(tparams.both_epochs == 0):\n",
    "                    continue\n",
    "                elif(tparams.spline_epochs == 0):\n",
    "                    if(tparams.relu_epochs > 0): # if we pretrained on ReLU, load those weights\n",
    "                        model = LinearBSpline(mparams.layers, mparams.cpoints, mparams.range_)\n",
    "                        model.load_state_dict(torch.load(f\"./temp_models/{model_save_code}.pt\", weights_only=True), strict=False)\n",
    "                    else:\n",
    "                        model = LinearBSpline(mparams.layers, mparams.cpoints, mparams.range_)\n",
    "                # else we just continue training on the same model\n",
    "                epochs = tparams.both_epochs\n",
    "\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters_no_deepspline(), lr=tparams.lr_wb)\n",
    "            aux_optimizer = optim.Adam(model.parameters_deepspline(), lr=tparams.lr_bs)\n",
    "\n",
    "            if(tparams.lrs == \"steplr\"):\n",
    "                # resetting scheduler\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=tparams.lrs_stepsize, gamma=tparams.lrs_gamma)\n",
    "                aux_scheduler = torch.optim.lr_scheduler.StepLR(aux_optimizer, step_size=tparams.lrs_stepsize, gamma=tparams.lrs_gamma)\n",
    "            \n",
    "            lmbda = 1e-4 # regularization weight\n",
    "            lipschitz = False # lipschitz control\n",
    "        \n",
    "        # training loop\n",
    "        length = epochs + (tparams.comp_relu if arch==\"relu\" else 0)\n",
    "\n",
    "        for epoch in range(length):\n",
    "            \n",
    "            # train the model\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_start = datetime.now()\n",
    "\n",
    "            # train over batches\n",
    "            for X_batch, y_batch in train_loader:\n",
    "\n",
    "                # forward pass + get loss\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                if(arch == \"relu\" or arch == \"both\"):\n",
    "                    optimizer.zero_grad()\n",
    "                if(arch == \"bspline\" or arch == \"both\"):\n",
    "                    aux_optimizer.zero_grad()\n",
    "                    if lipschitz is True:\n",
    "                        loss = loss + lmbda * model.BV2()\n",
    "                    else:\n",
    "                        loss = loss + lmbda * model.TV2()\n",
    "                epoch_loss += float(loss) * len(X_batch)\n",
    "\n",
    "                # compute gradient and step on the optimizer\n",
    "                loss.backward()\n",
    "                if(arch == \"relu\" or arch == \"both\"):\n",
    "                    optimizer.step()\n",
    "                if(arch == \"bspline\" or arch == \"both\"):\n",
    "                    aux_optimizer.step()                \n",
    "            \n",
    "            # step the LR scheduler\n",
    "            if(tparams.lrs != \"none\" and tparams.lrs != \"None\"):\n",
    "                if((arch == \"bspline\" or arch == \"both\") and scheduler.get_last_lr()[0] * tparams.lrs_gamma > 0.0001):\n",
    "                    scheduler.step()\n",
    "                if((arch==\"bspline\" or arch==\"both\") and aux_scheduler.get_last_lr()[0] * tparams.lrs_gamma > 0.00001):\n",
    "                    aux_scheduler.step()\n",
    "\n",
    "            # validation loss (on the whole val dataset)\n",
    "            model.eval()\n",
    "            y_pred = model(X_test) # pass in all the validation data\n",
    "            loss = float(loss_fn(y_pred, y_test)) #! could move this to after the next chunk to track times more accurately, but would be annoying to refactor for doing val loss after that\n",
    "\n",
    "            if(epoch % 5 == 0):\n",
    "                print(f\"Val loss on epoch {epoch}: {loss}\")\n",
    "\n",
    "            if(tparams.comp_relu > 0 and arch==\"relu\"): #\n",
    "                if(epoch == length - tparams.comp_relu - 1): # if we're at the switch point, save the model\n",
    "                    torch.save(model.state_dict(), f\"./temp_models/{model_save_code}.pt\")\n",
    "        \n",
    "        if(tparams.comp_relu == 0): # if not comp_relu, save at the end of the round of training\n",
    "            torch.save(model.state_dict(), f\"./temp_models/{model_save_code}.pt\")\n",
    "        \n",
    "        elif(arch == \"relu\"): # if it is comp_relu and we're on relu \n",
    "            start_time = time.perf_counter()\n",
    "            _ = model(X_test) # model output is irrelevant\n",
    "            end_time = time.perf_counter()\n",
    "            cr_fwd_lat = (end_time - start_time) / len(X_test) * 1000 * 1000 # per sample latency: seconds -> nanoseconds\n",
    "            cr_fwd_lat = round(cr_fwd_lat, 4)\n",
    "\n",
    "    # compute forward latency of end model\n",
    "    start_time = time.perf_counter()\n",
    "    _ = model(X_test) # model output is irrelevant\n",
    "    end_time = time.perf_counter()\n",
    "    fwd_lat = (end_time - start_time) / len(X_test) * 1000 * 1000 # per sample latency: seconds -> nanoseconds\n",
    "    fwd_lat = round(fwd_lat, 4)\n",
    "\n",
    "    final_locs = None\n",
    "    final_coeffs = None\n",
    "    if(tparams.spline_epochs > 0):\n",
    "        final_locs = model.get_deepspline_activations()[0]['locations']\n",
    "        final_coeffs = model.get_deepspline_activations()[0]['coefficients']\n",
    "\n",
    "    os.remove(f\"./temp_models/{model_save_code}.pt\")\n",
    "\n",
    "    return model, fwd_lat, final_locs, final_coeffs, cr_fwd_lat\n",
    "\n",
    "\n",
    "\n",
    "# By default, PyTorch attempts to use all available CPU cores for intra-op parallelism. Set threads = cpu cores\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "mparams = Config(\n",
    "    layers = [8],\n",
    "    cpoints = 3,\n",
    "    range_ = 1,\n",
    ")\n",
    "        \n",
    "tparams = Config(\n",
    "    relu_epochs = 50,\n",
    "    spline_epochs = 10,\n",
    "    both_epochs = 0,\n",
    "    comp_relu = 0,\n",
    "\n",
    "    batch_size = 10,\n",
    "    lr_wb = 0.001,\n",
    "    lr_bs = 0.0001,\n",
    "    lrs = 'none',\n",
    "    lrs_gamma = .9,\n",
    "    lrs_stepsize = 1,\n",
    ")\n",
    "\n",
    "model, fwd_lat, final_locs, final_coeffs, cr_fwd_lat = training_run(mparams, tparams, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSpline loss: 0.49540719389915466, fwd lat: 0.3711\n",
      "LSpline loss: 4.261738300323486, fwd lat: 0.4131\n"
     ]
    }
   ],
   "source": [
    "import linspline\n",
    "import importlib\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, shuffle=True)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "loss = float(loss_fn(model(X_test), y_test))\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "fwd_lat = (end_time - start_time) / len(X_test) * 1000 * 1000 #! NOT per sample latency: seconds -> nanoseconds\n",
    "fwd_lat = round(fwd_lat, 4)\n",
    "\n",
    "print(f\"BSpline loss: {loss}, fwd lat: {fwd_lat}\")\n",
    "\n",
    "# ~ ---------\n",
    "\n",
    "lin_model = linspline.LSplineFromBSpline(model.get_layers())\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "loss = float(loss_fn(lin_model(X_test), y_test))\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "fwd_lat = (end_time - start_time) / len(X_test) * 1000 * 1000 #! NOT per sample latency: seconds -> nanoseconds\n",
    "fwd_lat = round(fwd_lat, 4)\n",
    "print(f\"LSpline loss: {loss}, fwd lat: {fwd_lat}\")\n",
    "\n",
    "# may need to continue training on the linear splines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('../DeepSplines'))\n",
    "from deepsplines.ds_modules.deepBspline import DeepBSpline\n",
    "import deepsplines\n",
    "import utils\n",
    "\n",
    "layer_locs = []\n",
    "layer_coeffs = []\n",
    "for layer in model.get_layers():\n",
    "    if(type(layer) is deepsplines.ds_modules.deepBspline.DeepBSpline):\n",
    "        layer_locs.append(layer.grid_tensor.detach())\n",
    "        layer_coeffs.append(layer.coefficients_vect.view(layer.num_activations, layer.size).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'linspline.LinearSplineLayer'>\n"
     ]
    }
   ],
   "source": [
    "import linspline\n",
    "\n",
    "layer_locs2 = []\n",
    "layer_coeffs2 = []\n",
    "for layer in lin_model.get_layers():\n",
    "    if(type(layer) is not torch.nn.modules.linear.Linear):\n",
    "        print(type(layer))\n",
    "        (l2, c2) = layer.get_locs_coeffs()\n",
    "        \n",
    "        layer_locs2.append(l2)\n",
    "        layer_coeffs2.append(c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_layer(layer, input):\n",
    "    print(layer)\n",
    "    start_time = time.perf_counter()\n",
    "    _ = layer(input)\n",
    "    print((time.perf_counter() - start_time) * 1000 * 1000)\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU()\n",
      "486.4820512011647\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.5000,  1.0000,  5.0000,  8.0000, 10.0000]])\n",
      "\n",
      " ---- \n",
      "\n",
      "DeepBSpline(mode=fc, num_activations=8, init=relu, size=3, grid=1.0.)\n",
      "337.2259670868516\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.5000,  1.0000,  5.0000,  8.0000, 10.0000]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      " ---- \n",
      "\n",
      "LinearSplineLayer(8 locs, 8 coeffs, mode='fc')\n",
      "133.8960137218237\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.5000,  1.0000,  5.0000,  8.0000, 10.0000]])\n"
     ]
    }
   ],
   "source": [
    "from models import LinearReLU, LinearBSpline\n",
    "# importlib.reload(linspline)\n",
    "import linspline\n",
    "import torch\n",
    "\n",
    "\n",
    "input = torch.tensor([[-5, -1, 0, .5, 1, 5, 8, 10]]) # 1 input per activation function\n",
    "\n",
    "reluModel = LinearReLU([8])\n",
    "layer = reluModel.layers[1]\n",
    "run_layer(layer, input)\n",
    "\n",
    "print(\"\\n ---- \\n\")\n",
    "\n",
    "bsplineModel = LinearBSpline([8], 3, 1, \"relu\")\n",
    "layer = bsplineModel.get_layers()[1]\n",
    "run_layer(layer, input)\n",
    "\n",
    "print(\"\\n ---- \\n\")\n",
    "\n",
    "linModel = linspline.LSplineFromBSpline(bsplineModel.get_layers())\n",
    "layer = linModel.get_layers()[1]\n",
    "run_layer(layer, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init complete\n",
      "Epoch 0: 775293.4375\n",
      "Epoch 5: 2252.551025390625\n",
      "Epoch 10: 1283.8199462890625\n",
      "Epoch 15: 1187.6798095703125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from models import LinearBSpline\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# By default, PyTorch attempts to use all available CPU cores for intra-op parallelism. Set threads = cpu cores\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# train-test split of the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, shuffle=True)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# using a dataloader to randomize batching\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "\n",
    "lmbda = 1e-4 # regularization weight\n",
    "\n",
    "bsm = LinearBSpline([2], 3, 1, \"relu\")\n",
    "optimizer = optim.Adam(bsm.parameters_no_deepspline(), lr=0.001)\n",
    "aux_optimizer = optim.Adam(bsm.parameters_deepspline(), lr=0.0001)\n",
    "\n",
    "print(\"Init complete\")\n",
    "\n",
    "bsm.train()\n",
    "for epoch in range(20):\n",
    "    rloss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "\n",
    "        y_pred = bsm(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss2 = loss + lmbda * bsm.TV2()\n",
    "        loss2.backward()\n",
    "        optimizer.step()\n",
    "        aux_optimizer.step()\n",
    "        \n",
    "        rloss += loss\n",
    "    if(epoch % 5 == 0):\n",
    "        print(f\"Epoch {epoch}: {rloss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained BSpline\n",
      "DeepBSpline(mode=fc, num_activations=2, init=relu, size=3, grid=1.0.)\n",
      "time:  439.2910050228238\n",
      "tensor([[-0.2737, -0.0244],\n",
      "        [ 0.0382,  0.0082],\n",
      "        [ 1.8457,  0.0412]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import time, importlib, utils\n",
    "importlib.reload(linspline)\n",
    "importlib.reload(utils)\n",
    "\n",
    "def run_layer(layer, input):\n",
    "    # print(layer)\n",
    "    start_time = time.perf_counter()\n",
    "    _ = layer(input)\n",
    "    print(\"time: \", (time.perf_counter() - start_time) * 1000 * 1000)\n",
    "    print(_)\n",
    "\n",
    "# 3 inputs: [spline1, spline2]\n",
    "input = torch.tensor([[-2, -2], [0, 0], [2, 2]])\n",
    "\n",
    "print(\"Trained BSpline\")\n",
    "layer = bsm.get_layers()[1]\n",
    "\n",
    "layer_locs = layer.grid_tensor.detach()[0]\n",
    "layer_coeffs = layer.coefficients_vect.view(layer.num_activations, layer.size).detach()[0]\n",
    "\n",
    "# print(layer_locs)\n",
    "# print(layer_coeffs)\n",
    "\n",
    "# utils.plot_bspline(layer_locs, layer_coeffs, hide_bases=True)\n",
    "\n",
    "print(layer)\n",
    "\n",
    "run_layer(layer, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred LSpline\n",
      "time:  293.79199258983135\n",
      "tensor([[-0.2737, -0.0244],\n",
      "        [ 0.0382,  0.0082],\n",
      "        [ 1.8457,  0.0412]])\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(linspline)\n",
    "\n",
    "print(\"Transferred LSpline\")\n",
    "linModel = linspline.LSplineFromBSpline(bsm.get_layers())\n",
    "\n",
    "lin_layer = linModel.get_layers()[1]\n",
    "(linlocs, lincoeffs) = lin_layer.get_locs_coeffs()\n",
    "\n",
    "run_layer(lin_layer, input)\n",
    "# utils.plot_bspline(linlocs[0], lincoeffs[0], hide_bases=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init complete\n",
      "Epoch 0: 1173.4237060546875\n",
      "Epoch 1: 1279.8961181640625\n",
      "Epoch 2: 1145.614990234375\n",
      "Epoch 3: 1148.69677734375\n",
      "Epoch 4: 1183.909423828125\n",
      "Epoch 5: 1157.4105224609375\n",
      "Epoch 6: 1178.8897705078125\n",
      "Epoch 7: 1164.2030029296875\n",
      "Epoch 8: 1176.5655517578125\n",
      "Epoch 9: 1152.094482421875\n",
      "time:  320.02292573451996\n",
      "tensor([[-0.2737, -0.0244],\n",
      "        [ 0.0382,  0.0082],\n",
      "        [ 1.8457,  0.0412]])\n"
     ]
    }
   ],
   "source": [
    "lin_optimizer = optim.Adam(linModel.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Init complete\")\n",
    "\n",
    "linModel.train()\n",
    "for epoch in range(10):\n",
    "    rloss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        lin_optimizer.zero_grad()\n",
    "\n",
    "        y_pred = linModel(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        rloss += loss\n",
    "    print(f\"Epoch {epoch}: {rloss}\")\n",
    "\n",
    "layer = linModel.get_layers()[1]\n",
    "run_layer(layer, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LinearRelu\n",
    "\n",
    "relu = LinearRelu([8])\n",
    "optimizer = optim.Adam(bsm.parameters_no_deepspline(), lr=0.001)\n",
    "\n",
    "print(\"Init complete\")\n",
    "\n",
    "relu.train()\n",
    "for epoch in range(100):\n",
    "    rloss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = bsm(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss2 = loss + lmbda * bsm.TV2()\n",
    "        loss2.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        rloss += loss\n",
    "    if(epoch % 5 == 0):\n",
    "        print(f\"Epoch {epoch}: {rloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.linspace(-1, 1, 1000, dtype=torch.float)\n",
    "\n",
    "# Compute y values based on the new function\n",
    "y1 = layer(x)\n",
    "y2 = lin_layer(x)\n",
    "\n",
    "# # Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y1, label='BSpline', color='red', alpha = .5)\n",
    "plt.plot(x, y2, label='LSpline', color='blue', alpha = .5)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnn_liam_py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
