{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from itertools import accumulate\n",
    "import json\n",
    "\n",
    "def plot_seaborn2(\n",
    "        model_paths, aes=None, \n",
    "        palette=\"bright\", title=None, x=\"time\", y_ax=\"MSE Loss\", xlim=None, ylim=None, xmin=None, ymin=None, \n",
    "        aes1=\"Arch\", aes2=\"Layers\",\n",
    "        vbars=[], hide_legend=False, cutoff=None):\n",
    "    \n",
    "    sns.set_theme(\n",
    "        palette=palette,\n",
    "        style=\"whitegrid\",\n",
    "    )\n",
    "\n",
    "    # Data collection for plotting\n",
    "    plot_data = []\n",
    "\n",
    "    for i, m in enumerate(model_paths):\n",
    "        with open(f'saved/{m[1]}.json', 'r') as file:\n",
    "            model_data = json.load(file)\n",
    "\n",
    "        values = np.array(model_data[m[2]])\n",
    "\n",
    "        if m[3] == \"avg\":\n",
    "            plot_losses = np.mean(values, axis=0)\n",
    "        elif m[3] == \"med\":\n",
    "            plot_losses = np.median(values, axis=0)\n",
    "        elif m[3] == \"best\":\n",
    "            plot_losses = np.min(values, axis=0)\n",
    "\n",
    "        if(cutoff != None):\n",
    "            plot_losses = plot_losses[:cutoff]\n",
    "\n",
    "        print(f\"Avg over last 5 epochs on {aes[i][0]},{aes[i][1]},{m[3]}: {np.mean(plot_losses[-5])}\")\n",
    "        \n",
    "        for t, l in zip((range(len(plot_losses))), plot_losses):\n",
    "            plot_data.append({\n",
    "                \"Time\" if x == \"time\" else \"Epoch\": t,\n",
    "                y_ax: l,\n",
    "                aes1: aes[i][0],\n",
    "                aes2: aes[i][1],\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Plotting with Seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x_col = \"Time\" if x == \"time\" else \"Epoch\"\n",
    "\n",
    "    ax = sns.lineplot(data=df, x=x_col, y=y_ax, hue=aes1, style=aes2, markers=False)\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=14, pad=15)\n",
    "    \n",
    "    ax.set_xlabel(\"Time (s)\" if x == \"time\" else \"Epochs\", labelpad=10)\n",
    "    ax.set_ylabel(y_ax, labelpad=10)\n",
    "\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ymin if ymin is not None else 0, ylim)\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xmin if xmin is not None else 0, xlim)\n",
    "\n",
    "    # Add vertical bars\n",
    "    for (loc, col) in vbars:\n",
    "        plt.axvline(x=loc, color=col, linestyle=\"--\")\n",
    "\n",
    "    # Hide legend if needed\n",
    "    if hide_legend:\n",
    "        ax.legend_.remove()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "selector = \"med\"\n",
    "data = \"vals\"\n",
    "\n",
    "plot_seaborn2(\n",
    "    model_paths=[\n",
    "        \n",
    "        # (\"\", \"\", \"\", \"\"),\n",
    "        # (\"\", \"zeroed_[4]\", \"losses\", \"med\"),\n",
    "        # (\"\", \"zeroed_[8]\", \"losses\", \"med\"),\n",
    "\n",
    "    ],\n",
    "    aes=[\n",
    "        # (\"zeroed\", \"[4]\"),\n",
    "        (\"zeroed\", \"[8]\"),\n",
    "    ],\n",
    "    x = \"epochs\",\n",
    "    ylim = .8,\n",
    "    ymin = .2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "\n",
    "file = \"\"\n",
    "with open(f'saved/{file}.json', 'r') as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "print(torch.mean(model_data[\"zeroed_counter\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 1935.8859561152221\n",
      "Range: 8714 to 22\n",
      "[tensor([[10.0000,  0.0000,  0.0000,  2.0000,  0.4158,  0.4198,  0.7083,  0.1412]]), tensor([[3510.]])]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "bikes = pd.read_csv('bikesharing/day.csv') # not spatial\n",
    "filtered = bikes.copy()\n",
    "remove = [\"dteday\", \"yr\", \"instant\", \"casual\", \"registered\", \"season\", \"workingday\"]\n",
    "for column in remove:\n",
    "    filtered = filtered.drop(column, axis=1)\n",
    "\n",
    "data = filtered.drop(\"cnt\", axis=1)\n",
    "target = filtered[\"cnt\"]\n",
    "X, y = data.values, target.values\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compute standard deviation\n",
    "std = np.std(y)\n",
    "\n",
    "print(\"Standard Deviation:\", std)\n",
    "print(\"Range:\", np.max(y), \"to\", np.min(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, shuffle=True)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# using a dataloader to randomize batching\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "locs,coeffs = utils.train_models_count_zeroed_AFs(\n",
    "    runs = 1,\n",
    "    layers=[8], \n",
    "    epochs={\"relu\": 150, \"both\": 0, \"bspline\": 10}\n",
    ")\n",
    "\n",
    "layer_to_plot = 0\n",
    "utils.plot_bsplines(locs[layer_to_plot], coeffs[layer_to_plot], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regression7\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import importlib\n",
    "\n",
    "importlib.reload(regression7)\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../DeepSplines'))\n",
    "import deepsplines\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "run_summaries = []\n",
    "run_validations = [] # validations\n",
    "run_history = [] # training \n",
    "run_epoch_times = []\n",
    "outliers = 0\n",
    "\n",
    "# comp relu\n",
    "cr_run_summaries = []\n",
    "cr_run_validations = [] # validations\n",
    "cr_run_history = [] # training \n",
    "cr_run_epoch_times = []\n",
    "\n",
    "mparams = regression7.Config(\n",
    "    layers = [8],\n",
    "    cpoints = 3,\n",
    "    range_ = 1,\n",
    ")\n",
    "        \n",
    "tparams = regression7.Config(\n",
    "    epoch_specs = [\"150R\", \"10B\", \"50FB\", \"10B\", \"50FB\", \"10B\", \"50FB\", \"50L\"],\n",
    "    comp_relu = 0,\n",
    "\n",
    "    batch_size = 10,\n",
    "    lr_wb = 0.001,\n",
    "    lr_bs = 0.0001,\n",
    "    lr_ls = 0.0001,\n",
    "    lrs = \"none\",\n",
    "    lrs_gamma = .9,\n",
    "    lrs_stepsize = 5,\n",
    ")\n",
    "\n",
    "model, train_history, val_history, train_times, cr_val_history, cr_train_history, cr_train_times = regression7.training_run(mparams, tparams, X, y)\n",
    "\n",
    "run_validations.append(val_history)\n",
    "run_history.append(train_history)\n",
    "run_epoch_times.append(train_times) # microsec to seconds\n",
    "\n",
    "cr_run_validations.append(cr_val_history)\n",
    "cr_run_history.append(cr_train_history)\n",
    "cr_run_epoch_times.append(cr_train_times) # microsec to seconds\n",
    "\n",
    "data = {\n",
    "    \"mparams\": mparams.to_dict(),\n",
    "    \"tparams\": tparams.to_dict(),\n",
    "    \"times\": run_epoch_times,\n",
    "    \"vals\": run_validations,\n",
    "    \"trains\": run_history,\n",
    "    \"cr_times\": cr_run_epoch_times,\n",
    "    \"cr_vals\": cr_run_validations,\n",
    "    \"cr_trains\": cr_run_history,\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'saved/this mah test.json', 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regression7 import training_run, Config\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models import LinearBSpline\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "mparams = Config(\n",
    "    layers = [8],\n",
    "    cpoints = 3,\n",
    "    range_ = 1,\n",
    ")\n",
    "        \n",
    "tparams = Config(\n",
    "    epoch_specs = [\"150R\", \"10B\"],\n",
    "    comp_relu = 0,\n",
    "\n",
    "    batch_size = 10,\n",
    "    lr_wb = 0.001,\n",
    "    lr_bs = 0.0001,\n",
    "    lr_ls = 0.0001,\n",
    "    lr_fb = 0.0001,\n",
    "    lr_transfer_WBS = 0.0001,\n",
    "    lrs = \"none\",\n",
    "    lrs_gamma = .9,\n",
    "    lrs_stepsize = 5,\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "model, train_history, val_history, train_times2, cr_val_history, cr_train_history, cr_train_times2 = training_run(mparams, tparams, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-28 01:40:33,652] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-28 01:40:33,690] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wpatty/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../DeepSplines'))\n",
    "import deepsplines\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from models import LinearBSpline\n",
    "\n",
    "\n",
    "model = LinearBSpline([1], 3, 1)\n",
    "print(utils.get_model_zeroed_activations(model))\n",
    "\n",
    "# layer_locs = []\n",
    "# layer_coeffs = []\n",
    "\n",
    "# layer = model.get_layers()[1]\n",
    "# coeffs = layer.coefficients_vect.view(layer.num_activations, layer.size).detach()\n",
    "# layer_locs.append(layer.grid_tensor.detach())\n",
    "# layer_coeffs.append(coeffs)\n",
    "# layer_to_plot = 0\n",
    "# utils.plot_bsplines(layer_locs[layer_to_plot], layer_coeffs[layer_to_plot], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-1.,  0.,  1.]])]\n",
      "[tensor([[0., 0., 1.]])]\n"
     ]
    }
   ],
   "source": [
    "layer_locs = []\n",
    "layer_coeffs = []\n",
    "\n",
    "for layer in range(len(model.get_deepspline_activations())):\n",
    "    layer_locs.append(model.get_deepspline_activations()[layer]['locations'])\n",
    "    layer_coeffs.append(model.get_deepspline_activations()[layer]['coefficients'])\n",
    "\n",
    "print(layer_locs)\n",
    "print(layer_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m sel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlayer_locs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msel\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(layer_coeffs[layer][sel])\n\u001b[1;32m     10\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_bspline(layer_locs[layer][sel], layer_coeffs[layer][sel])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "layer = 0\n",
    "sel = 3\n",
    "\n",
    "print(layer_locs[layer][sel])\n",
    "print(layer_coeffs[layer][sel])\n",
    "\n",
    "utils.plot_bspline(layer_locs[layer][sel], layer_coeffs[layer][sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = final_locs[sel]\n",
    "y = final_coeffs[sel]\n",
    "\n",
    "x_new = np.linspace(x.min(), x.max(), 100)\n",
    "\n",
    "y_new = np.interp(x_new, x, y)\n",
    "plt.scatter(x, y, color='red', label='Original Data')\n",
    "plt.plot(x_new, y_new, color='blue', label='Interpolated Curve')\n",
    "\n",
    "plt.title('Piecewise Linear Interpolation with numpy.interp')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from regression7 import Config, training_run\n",
    "\n",
    "\n",
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "mparams = Config(\n",
    "    layers = [8],\n",
    "    cpoints = 3,\n",
    "    range_ = 1,\n",
    ")\n",
    "        \n",
    "tparams = Config(\n",
    "    relu_epochs = 200,\n",
    "    bspline_epochs = 50,\n",
    "    both_epochs = 0,\n",
    "    lspline_epochs = 0,\n",
    "    \n",
    "    batch_size = 10,\n",
    "    lr_wb = 0.001,\n",
    "    lr_bs = 0.0001,\n",
    "    lr_ls = 0.0001,\n",
    "    lrs = \"none\",\n",
    "    lrs_gamma = .9,\n",
    "    lrs_stepsize = 5,\n",
    "    comp_relu = 0,\n",
    "    bspline_order = [\"bspline\", \"both\"],\n",
    ")\n",
    "\n",
    "print(\"Init complete.\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "model, train_history, val_history, train_times, cr_val_history, cr_train_history, cr_train_times = training_run(mparams, tparams, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import accumulate\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"bright\")\n",
    "\n",
    "plot_data = []\n",
    "\n",
    "values = np.array(val_history)\n",
    "\n",
    "x = \"epochs\"\n",
    "\n",
    "for idx, l in enumerate(values):\n",
    "    plot_data.append({\n",
    "        \"Epoch\": idx,\n",
    "        \"MSE Loss\": l,\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Plotting with Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.lineplot(data=df, x=\"Epoch\", y=\"MSE Loss\", markers=False)\n",
    "ax.set_xlabel(\"Epochs\", labelpad=10)\n",
    "ax.set_ylabel(\"MSE Loss\", labelpad=10)\n",
    "\n",
    "ax.set_ylim(.3, 1)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.06, 10.4, 8.47, 2.11]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"RB\"\n",
    "# model = \"[8]_(0.001,0.0001,0.0001)_(['150R', '10B'])_100\"\n",
    "# model = \"[24, 8]_(0.001,0.0001,0.0001)_(['150R', '10B'])_100\"\n",
    "model = \"[24, 48, 24, 8]_(0.001,0.0001,0.0001)_(['150R', '10B'])_100\"\n",
    "\n",
    "with open(f'saved/{path}/{model}.json', 'r') as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "    sums = [0] * len(model_data[\"zeroed\"][0])\n",
    "    for run in model_data[\"zeroed\"]:\n",
    "        for l_idx,l_val in enumerate(run):\n",
    "            sums[l_idx] += l_val\n",
    "    \n",
    "    num_runs = len(model_data[\"zeroed\"])\n",
    "    print([x / num_runs for x in sums])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnn_liam_py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
