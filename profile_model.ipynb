{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-16 14:50:53,595] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-16 14:50:53,634] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wpatty/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "import utils, importlib\n",
    "from models import LinearReLU, LinearBSpline\n",
    "import linspline\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import time, torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "def profile_model(arch, layers, ctrl=3, range_=1, input_size=8, batch_size=10):\n",
    "\n",
    "    if(arch == \"ReLU\"):\n",
    "        model = LinearReLU(layers)\n",
    "    elif(arch == \"BSpline\"):\n",
    "        model = LinearBSpline(layers, ctrl, range_)\n",
    "    elif(arch == \"LSpline\"):\n",
    "        model = linspline.LSplineFromBSpline(LinearBSpline(layers, ctrl, range_).get_layers())\n",
    "\n",
    "    #^ FLOPs and Params\n",
    "\n",
    "    base_flops, macs, params = get_model_profile(model=model, # model\n",
    "            input_shape=(batch_size, input_size), # input shape to the model. If specified, the model takes a tensor with this shape as the only positional argument.\n",
    "            args=None, # list of positional arguments to the model.\n",
    "            kwargs=None, # dictionary of keyword arguments to the model.\n",
    "            print_profile=False, #! prints the model graph with the measured profile attached to each module\n",
    "            detailed=True, # print the detailed profile\n",
    "            module_depth=-1, # depth into the nested modules, with -1 being the inner most modules\n",
    "            top_modules=1, # the number of top modules to print aggregated profile\n",
    "            warm_up=10, # the number of warm-ups before measuring the time of each module\n",
    "            as_string=True, # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)\n",
    "            output_file=None, # path to the output file. If None, the profiler prints to stdout.\n",
    "            ignore_modules=None) # the list of modules to ignore in the profiling\n",
    "    \n",
    "    if(base_flops.__contains__(\"K\")):\n",
    "        flops = float(base_flops.replace(\"K\", \"\").strip()) * 1000 / batch_size\n",
    "    elif(base_flops.__contains__(\"M\")):\n",
    "        flops = float(base_flops.replace(\"M\", \"\").strip()) * 1000000 / batch_size\n",
    "\n",
    "    if(arch == \"BSpline\"):\n",
    "        flops += utils.calc_bspline_flops(model) #! this might be incorrect- need to check w the implementation\n",
    "    if(arch == \"LSpline\"):\n",
    "        flops += utils.calc_lspline_flops(model)\n",
    "    \n",
    "    housing = fetch_california_housing()\n",
    "\n",
    "    \n",
    "    #^ Forward Latency\n",
    "\n",
    "    per = 1000 # number of sims\n",
    "    \n",
    "    inpts = [ torch.rand(batch_size, input_size) * 3 - 1.5 for _ in range(per) ]\n",
    "    start_time = time.perf_counter()\n",
    "    for i in range(per):\n",
    "        _ = model(inpts[i]) # model output is irrelevant\n",
    "    end_time = time.perf_counter()\n",
    "    fwd_lat_sim = round((end_time - start_time) * 1000 * 1000 / per / batch_size, 2) # per sample latency: seconds -> microsec per input\n",
    "\n",
    "    \n",
    "    X, y = torch.tensor(housing.data, dtype=torch.float32), torch.tensor(housing.target, dtype=torch.float32).reshape(-1, 1)\n",
    "    # using a dataloader to randomize batching\n",
    "    train_dataset = TensorDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    inpts = []\n",
    "    for i in range(per):\n",
    "        inpts.append(next(iter(train_loader))[0]) # pre-emptively do this so it doesn't affect timing\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    for i in range(per):\n",
    "        _ = model(inpts[i]) # model output is irrelevant\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    fwd_lat_real = round((end_time - start_time) * 1000 * 1000 / per / batch_size, 2) # per sample latency: seconds -> microsec per input\n",
    "    \n",
    "    return flops, params, fwd_lat_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+---------------+------------+\n",
      "| Model                       | Params   |   FLOPs/input |   μs/input |\n",
      "+=============================+==========+===============+============+\n",
      "| ReLU [8]                    | 81       |           152 |       4.16 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| ReLU [24, 8]                | 425      |           816 |       9.71 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| ReLU [24, 48, 24, 8]        | 2.8 K    |          5496 |      17.53 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| ReLU [8, 48, 192, 48, 8]    | 19.58 K  |         38848 |      21.92 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| ReLU [24, 48, 96, 24, 8]    | 8.66 K   |         17112 |      22.43 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| BSpline [8]                 | 105      |           432 |      30.1  |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| BSpline [24, 8]             | 521      |          1936 |      63.94 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| BSpline [24, 48, 24, 8]     | 3.11 K   |          9136 |     128.92 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| BSpline [24, 48, 96, 24, 8] | 9.26 K   |         24112 |     167.95 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| LSpline [8]                 | 129      |           312 |      14.17 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| LSpline [24, 8]             | 617      |          1456 |      29.97 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| LSpline [24, 48, 24, 8]     | 3.42 K   |          7576 |      64.25 |\n",
      "+-----------------------------+----------+---------------+------------+\n",
      "| LSpline [24, 48, 96, 24, 8] | 9.86 K   |         21112 |      79.31 |\n",
      "+-----------------------------+----------+---------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from tabulate import tabulate\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(linspline)\n",
    "\n",
    "print(\"FLOPS may be inaccurate for LSpline\")\n",
    "print(\"LSpline params should be = bspline params\")\n",
    "\n",
    "archs_layers = [\n",
    "\n",
    "    #^ Testing number of control points:\n",
    "\n",
    "    # (\"BSpline\", [8]),\n",
    "    # (\"BSpline\", [24, 8]),\n",
    "    # (\"BSpline\", [24, 48, 24, 8]),\n",
    "    # (\"BSpline\", [24, 48, 96, 48, 24, 8]),\n",
    "\n",
    "    #^ Comparing across architectures\n",
    "\n",
    "    (\"ReLU\", [8]),\n",
    "    (\"ReLU\", [24, 8]),\n",
    "    (\"ReLU\", [24, 48, 24, 8]),\n",
    "    (\"ReLU\", [8, 48, 192, 48, 8]),\n",
    "    (\"ReLU\", [24, 48, 96, 24, 8]),\n",
    "\n",
    "    (\"BSpline\", [8]),\n",
    "    (\"BSpline\", [24, 8]),\n",
    "    (\"BSpline\", [24, 48, 24, 8]),\n",
    "    (\"BSpline\", [24, 48, 96, 24, 8]),\n",
    "\n",
    "    (\"LSpline\", [8]),\n",
    "    (\"LSpline\", [24, 8]),\n",
    "    (\"LSpline\", [24, 48, 24, 8]),\n",
    "    (\"LSpline\", [24, 48, 96, 24, 8]),\n",
    "\n",
    "    #^ Controlling for parameters:\n",
    "    # (\"BSpline\", [8]),\n",
    "    # (\"ReLU\", [11]),\n",
    "\n",
    "    # (\"BSpline\", [24, 8]),\n",
    "    # (\"ReLU\", [30, 8]),\n",
    "\n",
    "    # (\"BSpline\", [24, 48, 24, 8]),\n",
    "    # (\"ReLU\", [26, 50, 26, 8]),\n",
    "\n",
    "    # (\"BSpline\", [24, 48, 96, 48, 24, 8]),\n",
    "    # (\"ReLU\", [26, 50, 98, 50, 26, 8]),\n",
    "\n",
    "    #^ Controlling for FLOPS\n",
    "    #! Why doesn't FLOPs correspond to forward latency?\n",
    "    # (\"BSpline\", [8]),\n",
    "    # (\"ReLU\", [24]),\n",
    "\n",
    "    # (\"BSpline\", [24, 8]),\n",
    "    # (\"ReLU\", [64, 8]),\n",
    "\n",
    "    # (\"BSpline\", [24, 48, 24, 8]),\n",
    "    # (\"ReLU\", [24, 96, 24, 8]),\n",
    "\n",
    "    # (\"BSpline\", [24, 48, 96, 48, 24, 8]),\n",
    "    # (\"ReLU\", [24, 48, 144, 48, 24, 8]),\n",
    "\n",
    "    #^ Controlling for fwd lat\n",
    "\n",
    "    # (\"BSpline\", [8]),\n",
    "    # (\"ReLU\", [16, 32, 64, 128, 64, 32, 16, 8]),\n",
    "\n",
    "    # (\"BSpline\", [16, 8]),\n",
    "    # (\"ReLU\", [24, 48, 96, 192, 384, 576, 384, 192, 96, 48, 24, 8]),\n",
    "\n",
    "    # (\"BSpline\", [24, 48, 24, 8]),\n",
    "    # (\"ReLU\", [24, 48, 192, 768, 1152, 2304, 1152, 768, 192, 48, 24, 8]),\n",
    "\n",
    "]\n",
    "\n",
    "store = []\n",
    "\n",
    "# for ctrl in [3,5,11,23,55,111]:\n",
    "ctrl=3\n",
    "\n",
    "for (arch, layers) in archs_layers:\n",
    "    try:\n",
    "        flops, params, fwd_lat_real = profile_model(arch, layers, ctrl=ctrl)\n",
    "        store.append([f\"{arch} {layers}\", params, flops, fwd_lat_real])\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e, \"on \", (arch, layers))\n",
    "        store.append([f\"{arch} {layers}\", f\"{e}\", 0, 0])\n",
    "        \n",
    "    clear_output()\n",
    "\n",
    "headers = [\"Model\", \"Params\", \"FLOPs/input\", \"μs/input\"]\n",
    "print(tabulate(store, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linspline\n",
    "import importlib\n",
    "\n",
    "importlib.reload(linspline)\n",
    "\n",
    "print(profile_model(\"BSpline\", [8]))\n",
    "print(profile_model(\"LSpline\", [8]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnn_liam_py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
